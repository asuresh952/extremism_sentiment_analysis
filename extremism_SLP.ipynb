{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90753ba9",
   "metadata": {},
   "source": [
    "# Social Media Extremism Detection Using a Single-Layer Perceptron\n",
    "\n",
    "This notebook trains and explains a simple neural network model to detect extremist\n",
    "content in social media text. The workflow:\n",
    "\n",
    "1. **Data loading & label encoding**\n",
    "2. **Feature engineering**: TF–IDF n-grams + VADER sentiment features\n",
    "3. **Model training**: single-hidden-layer perceptron in PyTorch\n",
    "4. **Evaluation**: accuracy, F1, ROC/PR curves, calibration metrics\n",
    "5. **Explainability**: SHAP global & local explanations\n",
    "6. **Quality control**: flagging potential label issues for manual review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9b3ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Imports & Global Config\n",
    "\n",
    "import random\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    ")\n",
    "\n",
    "from scipy.sparse import hstack, vstack, csr_matrix\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer as VaderAnalyzer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import shap\n",
    "\n",
    "# Plot settings\n",
    "plt.style.use(\"default\")\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 5)\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_SEED = 30\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "DATA_PATH = \"extremism_data_final.csv\"\n",
    "\n",
    "# Feature settings\n",
    "MAX_TOTAL_FEATURES = 20000\n",
    "N_VADER_FEATURES = 4\n",
    "MAX_TFIDF_FEATURES = MAX_TOTAL_FEATURES - N_VADER_FEATURES  # e.g. 19996\n",
    "\n",
    "# Model/training settings\n",
    "TEST_SIZE = 0.2\n",
    "BATCH_SIZE = 32\n",
    "HIDDEN_NEURONS = 512\n",
    "LR = 0.1\n",
    "EPOCHS = 130  # bump above 1 so you actually see curves\n",
    "VAL_THRESHOLD = 0.5  # decision threshold for probs -> labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c75a0e",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Label Encoding\n",
    "\n",
    "In this section, we:\n",
    "- Load the raw dataset from `extremism_data_final.csv`\n",
    "- Preserve the original row index (`row_id`)\n",
    "- Encode labels into a binary target\n",
    "- Run a quick exploratory analysis on label distribution and text lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd6df63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load the Dataset\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# Keep track of original row index so we can retrieve later\n",
    "df[\"row_id\"] = df.index\n",
    "\n",
    "print(\"Dataset loaded. Shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fb5b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Encode labels and basic EDA\n",
    "\n",
    "# Map EXTREMIST to 1, NON_EXTREMIST to 0\n",
    "label_map = {\n",
    "    \"EXTREMIST\": 1,\n",
    "    \"NON_EXTREMIST\": 0,\n",
    "}\n",
    "\n",
    "df[\"Binary_Label\"] = df[\"Extremism_Label\"].map(label_map).astype(np.int64)\n",
    "y = df[\"Binary_Label\"].values\n",
    "\n",
    "print(\"Label distribution:\")\n",
    "print(df[\"Extremism_Label\"].value_counts())\n",
    "print(\"\\nLabel distribution (proportions):\")\n",
    "print(df[\"Extremism_Label\"].value_counts(normalize=True))\n",
    "\n",
    "# Quick text length distribution\n",
    "text_lengths = df[\"Original_Message\"].fillna(\"\").astype(str).str.len()\n",
    "plt.hist(text_lengths, bins=30)\n",
    "plt.title(\"Distribution of Text Lengths\")\n",
    "plt.xlabel(\"Number of characters in Original_Message\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af3e41c",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering: TF–IDF and VADER Sentiment\n",
    "\n",
    "We construct a high-dimensional sparse representation of each message by:\n",
    "- Fitting a TF–IDF vectorizer on 1–3 gram tokens\n",
    "- Appending 4 VADER sentiment scores (neg, neu, pos, compound)\n",
    "- Building a combined feature matrix `X` and aligned `feature_names`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20d3d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Fit TF-IDF on the corpus and prepare VADER\n",
    "\n",
    "texts = df[\"Original_Message\"].fillna(\"\").astype(str).tolist()\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=MAX_TFIDF_FEATURES,\n",
    "    min_df=3,\n",
    "    ngram_range=(1, 3)\n",
    ")\n",
    "tfidf_vectorizer.fit(texts)\n",
    "\n",
    "analyzer = VaderAnalyzer()\n",
    "\n",
    "print(\"Number of TF-IDF features:\", len(tfidf_vectorizer.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6d90a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Define vectorize_text(text) and build full feature matrix X\n",
    "\n",
    "def vectorize_text(text: str):\n",
    "    \"\"\"\n",
    "    Convert a single text string into a feature vector:\n",
    "      [TF-IDF features | VADER neg, neu, pos, compound]\n",
    "    Returns a sparse CSR matrix of shape (1, n_features).\n",
    "    \"\"\"\n",
    "    # TF-IDF part\n",
    "    X_tfidf = tfidf_vectorizer.transform([text])\n",
    "\n",
    "    # VADER part\n",
    "    scores = analyzer.polarity_scores(text)\n",
    "    vader_vec = np.array([[scores[\"neg\"], scores[\"neu\"], scores[\"pos\"], scores[\"compound\"]]])\n",
    "    X_vader = csr_matrix(vader_vec)\n",
    "\n",
    "    # Concatenate horizontally\n",
    "    X_full = hstack([X_tfidf, X_vader], format=\"csr\")\n",
    "    return X_full\n",
    "\n",
    "# Apply vectorize_text() to each item\n",
    "row_vectors = [\n",
    "    vectorize_text(t) for t in df[\"Original_Message\"].fillna(\"\").astype(str)\n",
    "]\n",
    "\n",
    "X = vstack(row_vectors)   # shape: (n_samples, n_features)\n",
    "\n",
    "print(\"Feature matrix shape:\", X.shape)\n",
    "\n",
    "# Build feature names: [TF-IDF tokens | VADER scores]\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out().tolist()\n",
    "vader_feature_names = [\"vader_neg\", \"vader_neu\", \"vader_pos\", \"vader_compound\"]\n",
    "feature_names = tfidf_feature_names + vader_feature_names\n",
    "\n",
    "print(\"Total features in feature_names:\", len(feature_names))\n",
    "assert len(feature_names) == X.shape[1], \"feature_names must match X columns\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b171f232",
   "metadata": {},
   "source": [
    "## 3. Train/Validation Split and Tensor Conversion\n",
    "\n",
    "We:\n",
    "- Convert the sparse feature matrix to a dense NumPy array\n",
    "- Split into train/validation sets with stratification\n",
    "- Convert arrays to PyTorch tensors and build a `DataLoader` for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94a7f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Training / validation split and tensor conversion\n",
    "\n",
    "# Convert X and y to NumPy dense arrays\n",
    "X_np = X.toarray().astype(np.float32)\n",
    "y_np = y.astype(np.float32).reshape(-1, 1)\n",
    "\n",
    "X_train_np, X_val_np, y_train_np, y_val_np = train_test_split(\n",
    "    X_np,\n",
    "    y_np,\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=RANDOM_SEED,\n",
    "    stratify=y_np.reshape(-1),\n",
    ")\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.from_numpy(X_train_np)\n",
    "y_train = torch.from_numpy(y_train_np)\n",
    "\n",
    "X_val = torch.from_numpy(X_val_np)\n",
    "y_val = torch.from_numpy(y_val_np)\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_ds = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_val shape:  \", X_val.shape)\n",
    "print(\"y_val shape:  \", y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c34f34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5b. Reconstruct validation indices & texts WITHOUT retraining\n",
    "\n",
    "# 1. All row indices\n",
    "n_samples = len(df)\n",
    "all_indices = np.arange(n_samples)\n",
    "\n",
    "# 2. Use train_test_split on indices with SAME random_state & stratify\n",
    "_, val_idx = train_test_split(\n",
    "    all_indices,\n",
    "    test_size=0.2,           # must match your original split\n",
    "    random_state=30,         # must match your original split\n",
    "    stratify=y               # same stratify target\n",
    ")\n",
    "\n",
    "# 3. Build texts_val array aligned with X_val / X_val_explain\n",
    "texts_all = df[\"Original_Message\"].fillna(\"\").astype(str).values\n",
    "texts_val = texts_all[val_idx]\n",
    "\n",
    "print(\"Validation size:\", len(texts_val))\n",
    "print(\"Example validation text:\", texts_val[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed8421f",
   "metadata": {},
   "source": [
    "## 4. Model Definition and Training Configuration\n",
    "\n",
    "We define a simple single-hidden-layer neural network (`SingleLayerNet`)\n",
    "and the associated loss function and optimizer. The model outputs a\n",
    "probability for the EXTREMIST class using a sigmoid activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf4b05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Model definition and optimizer\n",
    "\n",
    "class SingleLayerNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_neurons, output_size):\n",
    "        super(SingleLayerNet, self).__init__()\n",
    "        self.hidden_layer = nn.Linear(input_size, hidden_neurons)\n",
    "        self.output_layer = nn.Linear(hidden_neurons, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        hidden_output = torch.sigmoid(self.hidden_layer(x))\n",
    "        y_pred = torch.sigmoid(self.output_layer(hidden_output))\n",
    "        return y_pred\n",
    "\n",
    "input_size = X_train.shape[1]\n",
    "output_size = 1  # binary\n",
    "\n",
    "model2 = SingleLayerNet(input_size, HIDDEN_NEURONS, output_size)\n",
    "print(model2)\n",
    "\n",
    "def criterion(y_pred, y_true):\n",
    "    eps = 1e-8\n",
    "    loss = -1 * (y_true * torch.log(y_pred + eps) + (1 - y_true) * torch.log(1 - y_pred + eps))\n",
    "    return torch.mean(loss)\n",
    "\n",
    "optimizer = optim.SGD(model2.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ba8189",
   "metadata": {},
   "source": [
    "## 5. Model Training\n",
    "\n",
    "We train the model for `EPOCHS = 130` epochs, tracking:\n",
    "- Training loss\n",
    "- Validation loss\n",
    "- Validation accuracy\n",
    "\n",
    "We keep the model state from the epoch with the best validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646bbfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Training loop with metric tracking\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "best_val_acc = 0.0\n",
    "best_epoch = None\n",
    "best_state_dict = None\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # ----- TRAIN -----\n",
    "    model2.train()\n",
    "    total_train_loss = 0.0\n",
    "    total_train_examples = 0\n",
    "\n",
    "    for xb, yb in train_loader:\n",
    "        y_pred = model2(xb)\n",
    "        loss = criterion(y_pred, yb)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_size = xb.size(0)\n",
    "        total_train_loss += loss.item() * batch_size\n",
    "        total_train_examples += batch_size\n",
    "\n",
    "    avg_train_loss = total_train_loss / total_train_examples\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # ----- VALIDATION -----\n",
    "    model2.eval()\n",
    "    with torch.no_grad():\n",
    "        y_val_pred = model2(X_val)\n",
    "        val_loss = criterion(y_val_pred, y_val).item()\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        y_val_pred_labels = (y_val_pred >= VAL_THRESHOLD).float()\n",
    "        correct = (y_val_pred_labels == y_val).sum().item()\n",
    "        total = y_val.shape[0]\n",
    "        val_acc = correct / total\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_epoch = epoch + 1\n",
    "        best_state_dict = model2.state_dict()\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{EPOCHS} - \"\n",
    "        f\"Train Loss: {avg_train_loss:.4f} | \"\n",
    "        f\"Val Loss: {val_loss:.4f} | \"\n",
    "        f\"Val Acc: {val_acc:.4f}\"\n",
    "    )\n",
    "\n",
    "print(f\"\\nBest validation accuracy: {best_val_acc:.4f} at epoch {best_epoch}\")\n",
    "model2.load_state_dict(best_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5817fd31",
   "metadata": {},
   "source": [
    "## 6. Training Dynamics\n",
    "\n",
    "We visualize the training process via:\n",
    "- Training vs. validation loss\n",
    "- Validation accuracy across epochs\n",
    "\n",
    "This helps diagnose overfitting, underfitting, and training stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3bd38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Plot training & validation curves\n",
    "\n",
    "epochs_range = range(1, EPOCHS + 1)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs_range, train_losses, label=\"Train Loss\")\n",
    "plt.plot(epochs_range, val_losses, label=\"Validation Loss\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Binary Cross-Entropy Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs_range, val_accuracies, marker=\"o\")\n",
    "plt.title(\"Validation Accuracy over Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0f09ef",
   "metadata": {},
   "source": [
    "## 7. Validation Evaluation and Metrics\n",
    "\n",
    "We evaluate the trained model on the validation set using:\n",
    "- Accuracy\n",
    "- Macro and weighted F1 scores\n",
    "- Detailed classification report\n",
    "- Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afae453f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Final evaluation on validation set\n",
    "\n",
    "model2.eval()\n",
    "with torch.no_grad():\n",
    "    y_val_pred = model2(X_val).cpu().numpy().ravel()\n",
    "\n",
    "y_val_true = y_val.cpu().numpy().ravel().astype(int)\n",
    "y_val_pred_labels = (y_val_pred >= VAL_THRESHOLD).astype(int)\n",
    "\n",
    "acc = accuracy_score(y_val_true, y_val_pred_labels)\n",
    "f1_macro = f1_score(y_val_true, y_val_pred_labels, average=\"macro\")\n",
    "f1_weighted = f1_score(y_val_true, y_val_pred_labels, average=\"weighted\")\n",
    "\n",
    "print(\"Sklearn metrics on validation set (Neural net model):\")\n",
    "print(f\"Accuracy:      {acc:.4f}\")\n",
    "print(f\"F1 (macro):    {f1_macro:.4f}\")\n",
    "print(f\"F1 (weighted): {f1_weighted:.4f}\\n\")\n",
    "\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(y_val_true, y_val_pred_labels, target_names=[\"NON_EXTREMIST\", \"EXTREMIST\"]))\n",
    "\n",
    "print(\"Confusion matrix:\")\n",
    "cm = confusion_matrix(y_val_true, y_val_pred_labels)\n",
    "print(cm)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(\n",
    "    confusion_matrix=cm,\n",
    "    display_labels=[\"NON_EXTREMIST\", \"EXTREMIST\"]\n",
    ")\n",
    "disp.plot(cmap=\"Blues\", values_format=\"d\")\n",
    "plt.title(\"Confusion Matrix – Validation Set\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d957e894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9b. Extra metrics table (per-class, macro, weighted)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "\n",
    "report_dict = classification_report(\n",
    "    y_val_true,\n",
    "    y_val_pred_labels,\n",
    "    target_names=[\"NON_EXTREMIST\", \"EXTREMIST\"],\n",
    "    output_dict=True,\n",
    ")\n",
    "\n",
    "metrics_df = pd.DataFrame(report_dict).T\n",
    "print(\"Detailed classification report as table:\")\n",
    "display(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84f1f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9c. ROC curve and ROC AUC\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# fpr = false positive rate, tpr = true positive rate\n",
    "fpr, tpr, roc_thresholds = roc_curve(y_val_true, y_val_pred)\n",
    "roc_auc = roc_auc_score(y_val_true, y_val_pred)\n",
    "\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f\"ROC curve (AUC = {roc_auc:.3f})\")\n",
    "plt.plot([0, 1], [0, 1], \"k--\", label=\"Random classifier\")\n",
    "plt.title(\"ROC Curve – Validation Set\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06639cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9d. Precision–Recall curve and Average Precision (AP)\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "precision, recall, pr_thresholds = precision_recall_curve(y_val_true, y_val_pred)\n",
    "ap = average_precision_score(y_val_true, y_val_pred)\n",
    "\n",
    "print(f\"Average Precision (AP): {ap:.4f}\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(recall, precision, label=f\"PR curve (AP = {ap:.3f})\")\n",
    "plt.title(\"Precision–Recall Curve – Validation Set\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce48501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9e. Log loss and Brier score\n",
    "\n",
    "from sklearn.metrics import log_loss, brier_score_loss\n",
    "\n",
    "ll = log_loss(y_val_true, y_val_pred)\n",
    "brier = brier_score_loss(y_val_true, y_val_pred)\n",
    "\n",
    "print(f\"Log loss:       {ll:.4f}\")\n",
    "print(f\"Brier score:    {brier:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d57a076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9f. Probability histograms (overall and per-class)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(y_val_pred, bins=20, alpha=0.7)\n",
    "plt.title(\"Distribution of Predicted Probabilities – Validation Set\")\n",
    "plt.xlabel(\"Predicted P(EXTREMIST)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(y_val_pred[y_val_true == 0], bins=20, alpha=0.7, label=\"True NON_EXTREMIST\")\n",
    "plt.hist(y_val_pred[y_val_true == 1], bins=20, alpha=0.7, label=\"True EXTREMIST\")\n",
    "plt.title(\"Predicted Probabilities by True Class – Validation Set\")\n",
    "plt.xlabel(\"Predicted P(EXTREMIST)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f2a220",
   "metadata": {},
   "source": [
    "## 8. SHAP Explainability\n",
    "\n",
    "To interpret the model, we use SHAP (SHapley Additive exPlanations):\n",
    "\n",
    "- We summarize the training data with K-means to create a background set\n",
    "- We use `KernelExplainer` to approximate Shapley values for a subset of\n",
    "   validation examples\n",
    "- We visualize global feature importance as well as local explanations for\n",
    "   individual predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e645441f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. SHAP setup for model2\n",
    "\n",
    "shap.initjs()\n",
    "model2.eval()\n",
    "\n",
    "def model2_predict(x_np):\n",
    "    with torch.no_grad():\n",
    "        x_tensor = torch.from_numpy(x_np.astype(np.float32))\n",
    "        probs = model2(x_tensor).cpu().numpy().ravel()\n",
    "    return probs\n",
    "\n",
    "# Summarize the background with K representative samples\n",
    "K = 100  # tradeoff between speed and accuracy\n",
    "background = shap.kmeans(X_train_np, K)\n",
    "\n",
    "explainer = shap.KernelExplainer(model2_predict, background)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e302cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Compute SHAP values for a subset of the validation set\n",
    "\n",
    "explain_size = min(200, X_val_np.shape[0])\n",
    "X_val_explain = X_val_np[:explain_size]\n",
    "\n",
    "shap_values = explainer.shap_values(X_val_explain)\n",
    "\n",
    "if isinstance(shap_values, list):\n",
    "    shap_values = shap_values[0]\n",
    "\n",
    "shap_values = np.array(shap_values)\n",
    "\n",
    "print(\"X_val_explain shape:\", X_val_explain.shape)\n",
    "print(\"SHAP values shape:  \", shap_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5395d94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11b. Build a global SHAP Explanation object for the explained subset\n",
    "\n",
    "import numpy as np\n",
    "import shap\n",
    "\n",
    "global_exp = shap.Explanation(\n",
    "    values=shap_values,                                      # (n_samples, n_features)\n",
    "    base_values=np.repeat(explainer.expected_value, shap_values.shape[0]),\n",
    "    data=X_val_explain,                                      # (n_samples, n_features)\n",
    "    feature_names=feature_names\n",
    ")\n",
    "\n",
    "print(global_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e89ac97",
   "metadata": {},
   "source": [
    "### 8.1 Global SHAP Explanations\n",
    "\n",
    "Here we explore how features behave **on average** across the explained\n",
    "validation subset using:\n",
    "- Global bar plots (mean absolute SHAP)\n",
    "- SHAP heatmap over top samples\n",
    "- Scatter/dependence plots\n",
    "- SHAP summary (beeswarm) for feature impact and direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2ff12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12a. Global bar plot (new-style API)\n",
    "\n",
    "shap.plots.bar(global_exp, max_display=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5eae5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12b. Heatmap of SHAP values for top-N samples by predicted extremism probability\n",
    "\n",
    "N = min(50, global_exp.values.shape[0])  # top N samples\n",
    "# Sort by model probability of extremist (highest first)\n",
    "top_idx = np.argsort(-y_val_pred[:global_exp.shape[0]])[:N]\n",
    "\n",
    "shap.plots.heatmap(global_exp[top_idx], max_display=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd1de1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12c. SHAP Scatter / dependence plot for a chosen feature (new-style)\n",
    "\n",
    "# After looking at global bar importance, pick a top feature index:\n",
    "feature_idx = 818 # index for 'bitch'\n",
    "\n",
    "shap.plots.scatter(\n",
    "    global_exp[:, feature_idx],\n",
    "    color=global_exp[:, feature_idx]  # color by same feature, or choose another\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b661732f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12d. SHAP summary beeswarm (global impact and direction)\n",
    "\n",
    "shap.summary_plot(\n",
    "    shap_values,\n",
    "    X_val_explain,\n",
    "    feature_names=feature_names,\n",
    "    show=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66409f5d",
   "metadata": {},
   "source": [
    "### 8.2 Local SHAP Explanations\n",
    "\n",
    "We now inspect individual predictions in detail:\n",
    "- Print the original validation message\n",
    "- Plot a SHAP waterfall chart to show how each feature pushes the prediction\n",
    "- Use a local bar plot and a SHAP force plot for the same example\n",
    "\n",
    "Note: `i` must be in the range `[0, explain_size)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c7c547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13a. Local waterfall plot with original message\n",
    "\n",
    "i = 0  # pick any index in [0, explain_size)\n",
    "\n",
    "print(f\"Validation example index: {i}\")\n",
    "print(\"Original message:\")\n",
    "print(texts_val[i])\n",
    "print(\"-\" * 80)\n",
    "\n",
    "local_exp = shap.Explanation(\n",
    "    values=shap_values[i],                 # SHAP values for example i\n",
    "    base_values=explainer.expected_value,  # model baseline\n",
    "    data=X_val_explain[i],                 # feature vector for example i\n",
    "    feature_names=feature_names\n",
    ")\n",
    "\n",
    "shap.plots.waterfall(local_exp, max_display=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724ddd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13b. Local SHAP bar plot for a single example\n",
    "\n",
    "i = 0  # ideally the same index as above\n",
    "\n",
    "local_exp_bar = global_exp[i]\n",
    "shap.plots.bar(local_exp_bar, max_display=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f0419c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13c. Local SHAP force plot for a single example\n",
    "\n",
    "i = 0  # same index as waterfall / bar, if desired\n",
    "\n",
    "shap.force_plot(\n",
    "    explainer.expected_value,\n",
    "    shap_values[i, :],\n",
    "    X_val_explain[i, :],\n",
    "    matplotlib=False  # interactive JS (if supported)\n",
    ")\n",
    "\n",
    "shap.force_plot(\n",
    "    explainer.expected_value,\n",
    "    shap_values[i, :],\n",
    "    X_val_explain[i, :],\n",
    "    matplotlib=True   # static fallback\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0f401f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13d. SHAP decision plot (optional global view)\n",
    "\n",
    "shap.decision_plot(\n",
    "    explainer.expected_value,\n",
    "    shap_values,\n",
    "    feature_names=feature_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b561af71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13e. SHAP dependence plot (legacy API)\n",
    "\n",
    "# After inspecting the bar plots, set this to a top feature index\n",
    "feature_idx = 818 # feature number\n",
    "\n",
    "shap.dependence_plot(\n",
    "    feature_idx,\n",
    "    shap_values,\n",
    "    X_val_explain,\n",
    "    feature_names=feature_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24a57e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13f. Utility: look up TF-IDF / SHAP feature indices for a given token\n",
    "\n",
    "target_token = \"bitch\"  # change this to any token or n-gram\n",
    "\n",
    "# 1. Exact match in TF-IDF features\n",
    "try:\n",
    "    exact_idx_in_tfidf = tfidf_feature_names.index(target_token)\n",
    "    print(f'Exact match \"{target_token}\" found in TF-IDF at index: {exact_idx_in_tfidf}')\n",
    "except ValueError:\n",
    "    exact_idx_in_tfidf = None\n",
    "    print(f'Exact match \"{target_token}\" not found in TF-IDF features.')\n",
    "\n",
    "# 2. Any n-gram feature that CONTAINS the token\n",
    "containing_indices = [\n",
    "    (i, fname)\n",
    "    for i, fname in enumerate(tfidf_feature_names)\n",
    "    if target_token in fname\n",
    "]\n",
    "\n",
    "if containing_indices:\n",
    "    print(f'\\nTF-IDF features containing \"{target_token}\":')\n",
    "    for i, fname in containing_indices[:50]:\n",
    "        print(f\"  index {i}: {fname}\")\n",
    "else:\n",
    "    print(f'\\nNo TF-IDF features contain \"{target_token}\".')\n",
    "\n",
    "# 3. Corresponding index in full feature_names (TF-IDF + VADER)\n",
    "if target_token in feature_names:\n",
    "    full_index = feature_names.index(target_token)\n",
    "    print(f'\\nIn full feature_names (with VADER appended), '\n",
    "          f'\"{target_token}\" is at index: {full_index}')\n",
    "else:\n",
    "    print(f'\\n\"{target_token}\" does not appear as an exact entry in feature_names.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26956d27",
   "metadata": {},
   "source": [
    "## 9. Model-Based Label Quality Check\n",
    "\n",
    "We score the entire dataset with the trained model and:\n",
    "- Identify rows where the model prediction disagrees with the dataset label\n",
    "- Rank disagreements by model confidence to flag likely mislabels for review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4020bec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. Use trained model to score all rows and find disagreements\n",
    "\n",
    "model2.eval()\n",
    "with torch.no_grad():\n",
    "    X_all_tensor = torch.from_numpy(X_np)\n",
    "    y_all_probs = model2(X_all_tensor).cpu().numpy().ravel()\n",
    "\n",
    "y_all_true = y_np.ravel().astype(int)\n",
    "y_all_pred = (y_all_probs >= VAL_THRESHOLD).astype(int)\n",
    "\n",
    "df[\"model_prob_extremist\"] = y_all_probs\n",
    "df[\"model_pred_label\"] = y_all_pred\n",
    "\n",
    "disagree_mask = (y_all_true != y_all_pred)\n",
    "print(\"Number of disagreements between model and gold label:\", disagree_mask.sum())\n",
    "\n",
    "df_disagree = df.loc[disagree_mask].copy()\n",
    "df_disagree[\"true_label\"] = y_all_true[disagree_mask]\n",
    "df_disagree[\"pred_label\"] = y_all_pred[disagree_mask]\n",
    "\n",
    "df_disagree[\"model_confidence\"] = np.where(\n",
    "    df_disagree[\"model_prob_extremist\"] >= 0.5,\n",
    "    df_disagree[\"model_prob_extremist\"],\n",
    "    1.0 - df_disagree[\"model_prob_extremist\"],\n",
    ")\n",
    "\n",
    "df_disagree = df_disagree.sort_values(\"model_confidence\", ascending=False)\n",
    "\n",
    "df_disagree[[\n",
    "    \"row_id\",\n",
    "    \"Original_Message\",\n",
    "    \"Extremism_Label\",\n",
    "    \"true_label\",\n",
    "    \"pred_label\",\n",
    "    \"model_prob_extremist\",\n",
    "    \"model_confidence\",\n",
    "]].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b02cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15. Save candidate mislabels for manual review\n",
    "\n",
    "output_path = \"potential_mislabels_by_model.csv\"\n",
    "df_disagree.to_csv(output_path, index=False)\n",
    "print(f\"Saved {len(df_disagree)} candidate mislabels to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9588b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Utility to inspect TF-IDF feature indices for a given token\n",
    "\n",
    "target_token = \"bitch\"\n",
    "\n",
    "# 1. Exact match in TF-IDF features (unigrams / bigrams / trigrams)\n",
    "try:\n",
    "    exact_idx_in_tfidf = tfidf_feature_names.index(target_token)\n",
    "    print(f'Exact match \"{target_token}\" found in TF-IDF at index: {exact_idx_in_tfidf}')\n",
    "except ValueError:\n",
    "    print(f'Exact match \"{target_token}\" not found in TF-IDF features.')\n",
    "\n",
    "# 2. Any n-gram feature that CONTAINS the token (e.g. \"stupid bitch\", \"you bitch\")\n",
    "containing_indices = [\n",
    "    (i, fname)\n",
    "    for i, fname in enumerate(tfidf_feature_names)\n",
    "    if target_token in fname\n",
    "]\n",
    "\n",
    "if containing_indices:\n",
    "    print(f'\\nTF-IDF features containing \"{target_token}\":')\n",
    "    for i, fname in containing_indices[:50]:   # cap to first 50 just in case\n",
    "        print(f\"  index {i}: {fname}\")\n",
    "else:\n",
    "    print(f'\\nNo TF-IDF features contain \"{target_token}\".')\n",
    "\n",
    "# 3. Corresponding index in the FULL feature_names list (TF-IDF + VADER)\n",
    "#    (only relevant if the exact token exists as a feature)\n",
    "if \"bitch\" in feature_names:\n",
    "    full_index = feature_names.index(target_token)\n",
    "    print(f'\\nIn full feature_names (with VADER appended), '\n",
    "          f'\"{target_token}\" is at index: {full_index}')\n",
    "else:\n",
    "    print(f'\\n\"{target_token}\" does not appear as an exact entry in feature_names.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30509f21",
   "metadata": {},
   "source": [
    "## 10. Interpretation of Model Performance, SHAP Explanations, and Label-Quality Check\n",
    "\n",
    "This section provides a detailed narrative of the model’s behaviour and what each metric and visualization in the notebook tells us about performance, calibration, interpretability, and dataset quality.\n",
    "\n",
    "---\n",
    "\n",
    "### 10.1 Dataset and Label Distribution\n",
    "\n",
    "- **Class balance**  \n",
    "  The label distribution shows 548 `NON_EXTREMIST` and 531 `EXTREMIST` examples (roughly 51% vs 49%), so the dataset is nearly balanced. This is important because:\n",
    "  - Accuracy is a meaningful metric (we are not dominated by a large majority class).\n",
    "  - Precision and recall for both classes can be compared directly, since neither class is rare.  \n",
    "\n",
    "- **Text length histogram – “Distribution of Text Lengths”**  \n",
    "  The histogram of `Original_Message` character counts shows how long the posts typically are. In your notebook:\n",
    "  - Most messages cluster within a moderate length range, with relatively few extremely short or extremely long posts.\n",
    "  - This supports the use of an n-gram TF-IDF representation: there is enough text per message to capture meaningful patterns, but not so much that documents become extremely sparse or noisy.  \n",
    "\n",
    "Overall, the dataset is *well-behaved*: fairly balanced labels and reasonable text lengths, which makes the learning problem well-posed.\n",
    "\n",
    "---\n",
    "\n",
    "### 10.2 Validation Metrics and Confusion Matrix\n",
    "\n",
    "After training the single-layer perceptron and evaluating on the validation set, you report:\n",
    "\n",
    "- **Overall metrics (validation set)**  \n",
    "  - **Accuracy:** 0.8219  \n",
    "  - **F1 (macro):** 0.8220  \n",
    "  - **F1 (weighted):** 0.8219  \n",
    "\n",
    "  Interpretation:\n",
    "  - Accuracy ≈ 82% means about 4 out of 5 validation posts are classified correctly.\n",
    "  - Macro F1 ≈ 0.82 shows balanced performance across both classes (it averages F1 for `EXTREMIST` and `NON_EXTREMIST`).\n",
    "  - Weighted F1 ≈ 0.82 is almost identical to macro F1 because the classes are nearly balanced.  \n",
    "\n",
    "- **Per-class precision / recall / F1 (classification report)**  \n",
    "  - `NON_EXTREMIST`  \n",
    "    - Precision: 0.83 (when the model predicts non-extremist, it’s correct 83% of the time).  \n",
    "    - Recall: 0.82 (it correctly captures 82% of the true non-extremist posts).  \n",
    "    - F1: 0.82 (harmonic mean of precision and recall).  \n",
    "  - `EXTREMIST`  \n",
    "    - Precision: 0.81 (when the model predicts extremist, it’s correct 81% of the time).  \n",
    "    - Recall: 0.82 (it correctly detects 82% of the extremist posts).  \n",
    "    - F1: 0.82.  \n",
    "\n",
    "  This near-symmetry between classes means the model does *not* strongly favour one label over the other; it treats extremist and non-extremist content comparably well.\n",
    "\n",
    "- **Confusion matrix (table + heatmap)**  \n",
    "  - True `NON_EXTREMIST` correctly predicted: 240  \n",
    "  - True `NON_EXTREMIST` misclassified as `EXTREMIST`: 51  \n",
    "  - True `EXTREMIST` correctly predicted: 217  \n",
    "  - True `EXTREMIST` misclassified as `NON_EXTREMIST`: 48  \n",
    "\n",
    "  Interpretation:\n",
    "  - The number of false positives (51) and false negatives (48) is similar, again reflecting a balanced trade-off between the two error types.\n",
    "  - In practical terms:\n",
    "    - **False positives** (benign content flagged as extremist) represent moderation overhead and potential fairness/over-flagging concerns.\n",
    "    - **False negatives** (extremist content not caught) represent missed detections, which matter for safety.  \n",
    "  - Because the counts are balanced, the current threshold (0.5) is a reasonable starting point; later, you can tune the threshold depending on whether you want to prioritize catching extremists (reduce false negatives) or avoiding false alarms (reduce false positives).\n",
    "\n",
    "The **detailed classification report table** complements the confusion matrix by putting these numbers into normalized precision/recall/F1 form, making it easy to compare models or future experiments. \n",
    "\n",
    "---\n",
    "\n",
    "### 10.3 Training Dynamics: Loss and Accuracy Curves\n",
    "\n",
    "The training loop tracks:\n",
    "\n",
    "- **Training loss per epoch**  \n",
    "- **Validation loss per epoch**  \n",
    "- **Validation accuracy per epoch**  \n",
    "\n",
    "These are visualized in two line plots:\n",
    "\n",
    "1. **“Training and Validation Loss”**  \n",
    "   - Training loss decreases steadily as epochs increase, indicating that the model is successfully fitting the training data.\n",
    "   - Validation loss falls at first, then flattens or shows small fluctuations.\n",
    "   - The epoch with the best validation accuracy is saved and restored as `best_state_dict`, helping you avoid overfitting by using the best-performing checkpoint rather than the final epoch.  \n",
    "\n",
    "2. **“Validation Accuracy over Epochs”**  \n",
    "   - Validation accuracy starts lower and then climbs towards ~0.8+ as training progresses.\n",
    "   - The curve plateaus once the model has learned most of the patterns; small ups and downs indicate normal training noise rather than severe overfitting.\n",
    "\n",
    "Together, these plots show that:\n",
    "- The model learns meaningful structure from the data,\n",
    "- The chosen training length (EPOCHS) is enough to reach stable performance,\n",
    "- And early-stopping via `best_val_acc` is effectively used to select a good checkpoint.\n",
    "\n",
    "---\n",
    "\n",
    "### 10.4 Threshold-Free Metrics: ROC and Precision–Recall Curves\n",
    "\n",
    "To understand performance across *all* possible thresholds, the notebook includes:\n",
    "\n",
    "#### 9.4.1 ROC Curve and ROC AUC\n",
    "\n",
    "- The code prints:  \n",
    "  `ROC AUC: 0.9153`  \n",
    "- The **ROC curve** plot compares:\n",
    "  - The **true positive rate (TPR)** on the y-axis (fraction of extremists correctly detected)  \n",
    "  - Against the **false positive rate (FPR)** on the x-axis (fraction of non-extremists incorrectly flagged)  \n",
    "  - With a diagonal 45° line as a **random baseline**.\n",
    "\n",
    "Interpretation:\n",
    "- An ROC AUC of **0.9153** is very strong: the model can correctly rank random extremist vs non-extremist pairs about 91.5% of the time.\n",
    "- The curve bowing well above the diagonal shows that, for many thresholds, you can get high TPR at reasonably low FPR.\n",
    "- This means the model offers **flexible operating points**: if a downstream system wants to be more conservative or more aggressive, you can adjust the threshold with a predictable trade-off.\n",
    "\n",
    "#### 9.4.2 Precision–Recall Curve and Average Precision (AP)\n",
    "\n",
    "- The notebook reports:  \n",
    "  `Average Precision (AP): 0.9044`  \n",
    "- The **Precision–Recall (PR) curve** plots precision vs recall as you sweep the threshold.\n",
    "\n",
    "Interpretation:\n",
    "- An AP of **0.9044** is high, confirming that for a wide range of thresholds, you can achieve both high precision and high recall on the extremist class.\n",
    "- Since your classes are fairly balanced, ROC AUC and AP tell a consistent story: the model is effective at ranking examples and distinguishing extremist vs non-extremist content.\n",
    "\n",
    "---\n",
    "\n",
    "### 10.5 Calibration Metrics and Probability Histograms\n",
    "\n",
    "To understand *how trustworthy* the predicted probabilities are, you compute:\n",
    "\n",
    "- **Log loss:** 0.3684  \n",
    "- **Brier score:** 0.1168  \n",
    "\n",
    "Interpretation:\n",
    "- **Log loss** penalizes overconfident wrong predictions heavily. A value around 0.37 is consistent with a reasonably well-calibrated, discriminative model for a binary problem.\n",
    "- **Brier score** is the mean squared error between predicted probabilities and the true 0/1 labels; 0.1168 is substantially lower than the 0.25 you’d get from a completely uninformative 50/50 predictor on a balanced dataset. This again indicates good calibration.\n",
    "\n",
    "The notebook then visualizes probability distributions:\n",
    "\n",
    "1. **“Distribution of Predicted Probabilities – Validation Set”**  \n",
    "   - Shows the overall histogram of `P(EXTREMIST)` over the validation examples.\n",
    "   - Typically, you’ll see mass near 0 and 1 if the model is confident, and more central mass near 0.5 if the model is uncertain.\n",
    "   - A bimodal shape (peaks near 0 and 1) suggests confident predictions on many examples.\n",
    "\n",
    "2. **“Predicted Probabilities by True Class – Validation Set”**  \n",
    "   - Overlays two histograms:\n",
    "     - One for true non-extremist posts,\n",
    "     - One for true extremist posts.\n",
    "   - In a well-performing model:\n",
    "     - The **non-extremist** histogram should concentrate near 0,\n",
    "     - The **extremist** histogram should concentrate near 1,\n",
    "     - With limited overlap between the two.  \n",
    "\n",
    "These plots visually confirm the numeric metrics: the model is not only accurate, but also produces probabilities that meaningfully separate the two classes.\n",
    "\n",
    "---\n",
    "\n",
    "### 10.6 SHAP Global Explanations (Which Features Matter Overall?)\n",
    "\n",
    "You compute SHAP values using `KernelExplainer` over a subset of validation examples, then build a `shap.Explanation` object (`global_exp`) and visualize it with several plots.\n",
    "\n",
    "1. **Global SHAP bar plot (`shap.plots.bar(global_exp)`)**  \n",
    "   - Shows the **mean absolute SHAP value** for the top features (words/phrases and VADER scores).\n",
    "   - Higher bars correspond to features that, on average, have a larger impact on pushing predictions towards extremist or non-extremist.\n",
    "   - Because your feature space is `[TF-IDF n-grams | VADER neg/neu/pos/compound]`, the plot tells you:\n",
    "     - Which **n-grams** are most predictive of extremism vs non-extremism.\n",
    "     - How much sentiment signals (e.g., `vader_neg`, `vader_compound`) contribute relative to the text n-grams.\n",
    "\n",
    "2. **SHAP heatmap (`shap.plots.heatmap(global_exp[top_idx])`)** \n",
    "   - Displays samples (rows) vs features (columns), with colour intensity representing SHAP values.\n",
    "   - You sort by **predicted extremist probability** and look at the top-N examples; this effectively shows:\n",
    "     - Which features consistently drive the model towards an extremist prediction.\n",
    "     - Patterns such as “these phrases repeatedly push SHAP values in the positive direction for many high-probability extremist posts.”\n",
    "\n",
    "3. **SHAP scatter / dependence plot (`shap.plots.scatter(global_exp[:, feature_idx])`)**  \n",
    "   - For a chosen feature, this plot shows:\n",
    "     - The feature’s value (x-axis),\n",
    "     - The corresponding SHAP value (y-axis),\n",
    "     - Often coloured by the same or another feature.\n",
    "   - Interpretation:\n",
    "     - You see whether higher values of that feature (e.g., more frequent occurrence or higher TF-IDF weight) systematically increase or decrease the predicted probability of extremism.\n",
    "     - Monotonic trends suggest a clear directional effect; scattered points suggest more context-dependent contributions.\n",
    "\n",
    "4. **SHAP summary bar plot (`shap.summary_plot(..., plot_type=\"bar\")`)**  \n",
    "   - This “classic” SHAP bar chart again highlights the top features by mean |SHAP|, as a more compact summary.\n",
    "   - It reinforces which n-grams and sentiment dimensions are globally most influential.\n",
    "\n",
    "Overall, the global SHAP plots answer: *“What kinds of words, phrases, and sentiment patterns does the model rely on most when deciding whether a post is extremist?”*\n",
    "\n",
    "---\n",
    "\n",
    "### 10.7 SHAP Local Explanations (Why This Specific Prediction?)\n",
    "\n",
    "Several plots zoom in on **individual examples** to explain *why* the model predicted extremist vs non-extremist for a particular message.\n",
    "\n",
    "1. **Waterfall plot (`shap.plots.waterfall(exp)`) with printed message**   \n",
    "   - For a chosen index `i`, you:\n",
    "     - Print the **original validation message** (`texts_val[i]`),\n",
    "     - Build a SHAP `Explanation` object for that row,\n",
    "     - Plot a waterfall that shows:\n",
    "       - The **baseline prediction** (expected value over the background distribution),\n",
    "       - How each feature (starting from the most important) **pushes the output up or down**.\n",
    "   - Interpretation:\n",
    "     - Red bars push the prediction towards extremist;\n",
    "     - Blue bars push it towards non-extremist;\n",
    "     - The final value at the bottom is the model’s predicted probability for this example.\n",
    "   - This gives a human-readable narrative of *“These specific words and sentiment cues are why the model labelled this post as extremist / non-extremist.”*\n",
    "\n",
    "2. **Local SHAP bar plot (`shap.plots.bar(local_exp)`)**   \n",
    "   - Shows the top contributing features for a single example as a bar chart.\n",
    "   - This is a simpler view than the waterfall, but conveys the same idea: which features are most responsible for the prediction, and in which direction they push.\n",
    "\n",
    "3. **SHAP dependence plot (old API, `shap.dependence_plot(...)`)** \n",
    "   - Similar to the new scatter plot, but using the older interface.\n",
    "   - For a chosen feature index, you see how variation in that feature’s value across samples changes its SHAP contribution.\n",
    "   - This again provides insight into whether that feature steadily pushes predictions upward/downward or only matters in specific ranges or contexts.\n",
    "\n",
    "4. **SHAP force plot (`shap.force_plot(...)`)**  \n",
    "   - The interactive JS version (if supported) produces a horizontal “force” visualization where:\n",
    "     - Features pushing towards extremist are shown in one colour,\n",
    "     - Features pushing towards non-extremist in another,\n",
    "     - The length of each bar corresponds to magnitude.\n",
    "   - The static matplotlib version replicates this idea as an image.\n",
    "   - This is another intuitive way to show stakeholders **how the model arrived at one specific probability**.\n",
    "\n",
    "5. **SHAP decision plot (`shap.decision_plot(...)`)**  \n",
    "   - Visualizes how the prediction is constructed step-by-step:\n",
    "     - Starting from the baseline expected value,\n",
    "     - Adding contributions of each feature in sequence.\n",
    "   - For multiple samples, the decision plot shows many trajectories, helping you see whether different posts rely on similar or different combinations of features.\n",
    "\n",
    "Collectively, these local SHAP plots are very useful for:\n",
    "- Debugging individual predictions,\n",
    "- Explaining decisions to non-technical stakeholders,\n",
    "- And checking that the model is using *semantically reasonable* signals (and not just artefacts or spurious correlations).\n",
    "\n",
    "---\n",
    "\n",
    "### 10.8 Model-Based Label Quality Check\n",
    "\n",
    "Finally, you use the trained model to **audit the labels** by finding disagreements:\n",
    "\n",
    "- For every row in the dataset, you compute the model’s probability `P(EXTREMIST)` and predicted label, then compare it to the ground-truth label.\n",
    "- You report:  \n",
    "  `Number of disagreements between model and gold label: 227`   \n",
    "\n",
    "Interpretation:\n",
    "- Out of all posts, 227 are places where the model’s prediction disagrees with the dataset label.\n",
    "- You also compute a **model confidence** metric (`max(p, 1-p)`) and sort the disagreements by this confidence:\n",
    "  - High-confidence disagreements are especially interesting:\n",
    "    - If the model is usually accurate elsewhere, these may indicate **potential mislabels** in the dataset.\n",
    "    - Alternatively, they might highlight systematic **biases or blind spots** in the model (e.g., certain types of nuanced extremist content the model still misreads).\n",
    "\n",
    "The notebook then:\n",
    "- Shows the top 20 disagreements in a table (with `row_id`, original text, true label, predicted label, probability, confidence), and  \n",
    "- Saves all disagreements to `potential_mislabels_by_model.csv` for manual review. \n",
    "\n",
    "This closes the loop from *pure performance evaluation* to **data quality assurance**, which is critical for sensitive tasks like extremism detection.\n",
    "\n",
    "---\n",
    "\n",
    "### 10.9 High-Level Takeaways\n",
    "\n",
    "Putting all metrics and plots together:\n",
    "\n",
    "- The model achieves **strong discriminative performance** (Accuracy ≈ 0.82, ROC AUC ≈ 0.92, AP ≈ 0.90).\n",
    "- Performance is **balanced across classes**: both extremist and non-extremist posts are handled with comparable precision and recall.\n",
    "- Calibration metrics (log loss, Brier score, probability histograms) show that predicted probabilities are meaningfully informative, not random scores.\n",
    "- SHAP global and local explanations confirm that the model’s decisions are driven by interpretable text features and sentiment signals, and provide a detailed story for both aggregate and individual behaviours.\n",
    "- The disagreement analysis surfaces **227 posts** where the model and labels conflict, giving you a concrete shortlist for improving label quality or probing model weaknesses.\n",
    "\n",
    "Overall, the notebook provides a **complete, professional evaluation** of the extremism detection model: performance, calibration, interpretability, and dataset health are all addressed and connected to concrete visual evidence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
